#!/usr/bin/env python3
"""
å¹³è¡Œè¯­æ–™è´¨é‡è¯„ä¼°å™¨
ä½¿ç”¨Gemini APIè¯„ä¼°JSONLæ–‡ä»¶ä¸­çš„è‹±-è€å¹³è¡Œè¯­æ–™è´¨é‡ã€‚
"""
import os
import json
import argparse
import logging
from pathlib import Path
from typing import List, Dict
import time
import asyncio
import aiohttp
import pandas as pd

# --- è®¾ç½®æ—¥å¿— ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- ä½¿ç”¨ä¸Šé¢è®¾è®¡çš„Prompt ---
EVALUATION_PROMPT_TEMPLATE = """
As an expert linguistic evaluator specializing in English/Chinese and Thai/Lao, your task is to meticulously assess the quality of a Thai/Lao translation based on its English/Chinese source text.

**Evaluation Criteria:**

1.  **Accuracy (à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³):** Does the Lao text accurately and completely convey the meaning, nuance, and intent of the English/Chinese source? Are there any omissions, additions, or mistranslations?
2.  **Fluency & Grammar (à¸„à¸§à¸²à¸¡à¸¥à¸·à¹ˆà¸™à¹„à¸«à¸¥à¹à¸¥à¸°à¹„à¸§à¸¢à¸²à¸à¸£à¸“à¹Œ):** Is the Lao text grammatically correct? Does it sound natural and fluent to a native speaker? Is the phrasing awkward or unconventional?
3.  **Machine Translation (MT) Artifacts (à¸£à¹ˆà¸­à¸‡à¸£à¸­à¸¢à¸à¸²à¸£à¹à¸›à¸¥à¸”à¹‰à¸§à¸¢à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡):** Does the translation show signs of being a raw, unedited machine translation? For example, overly literal translations, unnatural word choices, or grammatical structures that mimic English/Chinese too closely.

**Your Task:**
Based on the criteria above, provide your analysis in Simple Chinese and a final quality score for the following text pair.

**Source Text (English/Chinese):**
"{source_text}"

**Target Text (Thai/Lao):**
"{target_text}"

**Output Format:**
You MUST provide your response in a valid JSON object format, with no other text or explanations before or after the JSON block. The JSON object must contain the following keys:
- "evaluation_summary": A brief, one-sentence summary of the translation quality.
- "accuracy_critique": A detailed critique of the translation's accuracy.
- "fluency_critique": A detailed critique of the translation's fluency and grammar.
- "mt_suspicion_critique": Your assessment of whether it sounds like a machine translation and why.
- "quality_score": A single integer score from 1 to 10, where 1 is "Completely incorrect/unusable" and 10 is "Perfect, human-quality translation".
"""

class GeminiEvaluator:
    """ä½¿ç”¨Gemini APIè¯„ä¼°è¯­æ–™çš„å¤„ç†å™¨"""
    def __init__(self, api_key: str, base_url: str = None, model: str = "gemini-2.5-flash-preview-05-20-nothinking"): # type: ignore
        self.api_key = api_key
        self.base_url = base_url or "https://one-api.modelbest.co/v1"
        self.model = model
        self.session = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def evaluate_pair(self, source_text: str, target_text: str) -> dict:
        """è¯„ä¼°å•ä¸ªæº-ç›®æ ‡æ–‡æœ¬å¯¹"""
        if not source_text or not target_text:
            return {"error": "Source or target text is empty."}
        
        prompt = EVALUATION_PROMPT_TEMPLATE.format(source_text=source_text, target_text=target_text)
        
        try:
            headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
            payload = {
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.1,
                "response_format": {"type": "json_object"} # å°è¯•å¼ºåˆ¶æ¨¡å‹è¾“å‡ºJSON
            }
            
            async with self.session.post(f"{self.base_url}/chat/completions", headers=headers, json=payload, timeout=120) as response: # type: ignore
                if response.status == 200:
                    response_json = await response.json()
                    content = response_json['choices'][0]['message']['content']
                    try:
                        # è§£ææ¨¡å‹è¿”å›çš„JSONå­—ç¬¦ä¸²
                        evaluation_data = json.loads(content)
                        logger.info(f"æˆåŠŸè¯„ä¼°å¹¶æ‰“åˆ†: {evaluation_data.get('quality_score')}/10")
                        return evaluation_data
                    except json.JSONDecodeError:
                        logger.error(f"æ— æ³•è§£ææ¨¡å‹è¿”å›çš„JSON: {content}")
                        return {"error": "Failed to parse JSON response", "raw_response": content}
                else:
                    error_text = await response.text()
                    logger.error(f"APIè¯·æ±‚å¤±è´¥: {response.status} - {error_text}")
                    return {"error": f"API Error {response.status}", "details": error_text}
        except Exception as e:
            logger.error(f"è¯„ä¼°æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            return {"error": "Exception during evaluation", "details": str(e)}

def get_jsonl_files(folder_path: str) -> List[Path]:
    folder = Path(folder_path)
    if not folder.is_dir():
        raise FileNotFoundError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
    files = sorted(list(folder.rglob('*.jsonl')))
    logger.info(f"åœ¨ '{folder_path}' ä¸­æ‰¾åˆ° {len(files)} ä¸ª .jsonl æ–‡ä»¶ã€‚")
    return files

async def process_file(evaluator: GeminiEvaluator, input_path: Path, output_path: Path, batch_size: int, delay: float):
    logger.info(f"--- ğŸ“‚ å¼€å§‹å¤„ç†æ–‡ä»¶: {input_path.name} ---")
    
    try:
        # ä½¿ç”¨pandasé€å—è¯»å–ï¼Œæ›´ç¨³å¥
        chunk_iterator = pd.read_json(input_path, lines=True, chunksize=batch_size)
        
        with open(output_path, 'w', encoding='utf-8') as f_out:
            for i, chunk in enumerate(chunk_iterator):
                logger.info(f"  - æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {i+1} (å…± {len(chunk)} è¡Œ)...")
                
                tasks = []
                for _, row in chunk.iterrows():
                    source = row.get("source_text", "")
                    target = row.get("target_text", "")
                    tasks.append(evaluator.evaluate_pair(source, target))
                
                evaluations = await asyncio.gather(*tasks)
                
                # åˆå¹¶åŸå§‹æ•°æ®å’Œè¯„ä¼°ç»“æœå¹¶å†™å…¥æ–‡ä»¶
                for index, original_data in chunk.to_dict('index').items():
                    evaluation_result = evaluations[index % batch_size]
                    # å°†è¯„ä¼°ç»“æœåˆå¹¶åˆ°æ–°å­—æ®µ "gemini_evaluation" ä¸­
                    original_data["gemini_evaluation"] = evaluation_result
                    f_out.write(json.dumps(original_data, ensure_ascii=False) + '\n')
                
                logger.info(f"  - æ‰¹æ¬¡ {i+1} å¤„ç†å®Œæˆå¹¶å·²å†™å…¥ã€‚")
                await asyncio.sleep(delay)

        logger.info(f"--- âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {output_path.name} ---")

    except Exception as e:
        logger.error(f"å¤„ç†æ–‡ä»¶ {input_path.name} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)


async def main():
    parser = argparse.ArgumentParser(description='ä½¿ç”¨Gemini APIæ‰¹é‡è¯„ä¼°å¹³è¡Œè¯­æ–™è´¨é‡')
    parser.add_argument('-i', '--input-folder', required=True, help='åŒ…å«.jsonlæ–‡ä»¶çš„è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('-o', '--output-folder', required=True, help='ç”¨äºå­˜æ”¾è¯„ä¼°ç»“æœçš„è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('-k', '--api-key', help='APIå¯†é’¥ (æˆ–é€šè¿‡ç¯å¢ƒå˜é‡ GEMINI_API_KEY è®¾ç½®)')
    parser.add_argument('-u', '--base-url', help='APIåŸºç¡€URL (ä¾‹å¦‚ OpenAI å…¼å®¹æ¥å£)')
    parser.add_argument('-m', '--model', default='gemini-2.5-flash-preview-05-20-nothinking', help='è¦ä½¿ç”¨çš„æ¨¡å‹åç§°')
    parser.add_argument('-b', '--batch-size', type=int, default=10, help='å¹¶å‘å¤„ç†çš„æ‰¹å¤§å°')
    parser.add_argument('-d', '--delay', type=float, default=1.0, help='æ¯æ‰¹è¯·æ±‚é—´çš„å»¶è¿Ÿ(ç§’)')
    args = parser.parse_args()

    api_key = args.api_key or os.getenv('GEMINI_API_KEY')
    if not api_key:
        logger.error("âŒ å¿…é¡»æä¾›APIå¯†é’¥! è¯·é€šè¿‡ --api-key å‚æ•°æˆ– GEMINI_API_KEY ç¯å¢ƒå˜é‡è®¾ç½®ã€‚")
        return

    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    output_dir = Path(args.output_folder)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    start_time = time.time()
    try:
        jsonl_files = get_jsonl_files(args.input_folder)
        if not jsonl_files:
            return

        async with GeminiEvaluator(api_key, args.base_url, args.model) as evaluator:
            for input_file in jsonl_files:
                # å®šä¹‰è¾“å‡ºæ–‡ä»¶å
                output_file = output_dir / f"{input_file.stem}_evaluated.jsonl"
                await process_file(evaluator, input_file, output_file, args.batch_size, args.delay)

    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡ŒæœŸé—´å‘ç”Ÿæœªæ•è·çš„é”™è¯¯: {e}")
    finally:
        end_time = time.time()
        logger.info(f"ğŸš€ å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f} ç§’ã€‚")

if __name__ == "__main__":
    asyncio.run(main())
